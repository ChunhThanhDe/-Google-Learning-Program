## The Early Years ⏳

🌱 The development of computing has been steadily growing since the invention of the analytical engine but didn't make a huge leap forward until World War II. Back then, research into computing was super expensive. Electronic components were large and you needed lots of them to compute anything of value. This also meant that computers took up a ton of space and many efforts were underfunded and unable to make headway.

## War and Computing 💥

💣 But when the war broke out, governments started pouring money and resources into computing research. They wanted to help develop technologies that would give them advantages over other countries. Lots of efforts were spun up and advancements were made in fields like cryptography. Cryptography is the art of writing and solving codes. During the war, computers were used to process secret messages from enemies faster than a human could ever hope to do. Today the role cryptography plays in secure communication is a critical part of computer security, which we'll learn more about in a later course.

## Post-War Progress 🚀

🌍 For now, let's look at how computers started to make a dramatic impact on society. After the war, companies like IBM, Hewlett Packard, and others were advancing their technologies into the academic, business, and government realms. Lots of technological advancements in computing were made in the 20th century. Thanks to direct interests from governments, scientists, and companies leftover from World War II. These organizations invented new methods to store data in computers, which fueled the growth of computational power.

## From Punch Cards to Magnetic Tape 📇

🃏 Consider this, until the 1950s punch cards were a popular way to store data. Operators would have decks of ordered punch cards that were used for data processing. If they dropped the deck by accident and the cards get out of order, it was almost impossible to get them sorted again, there were obviously some limitations to punch cards. But thanks to new technological innovations like magnetic tape and its counterparts, people began to store more data on more reliable media.

🎥 A magnetic tape worked by magnetizing data onto a tape. This left stacks and stacks of punch cards to collect dust, while the new magnetic tape counterparts began to revolutionize the industry. I wasn't joking when I said early computers took up a lot of space. They had huge machines to read data and racks of vacuum tubes that help move that data. Vacuum tubes controlled the electricity voltages and all electronic equipment like televisions and radios. But these specific vacuum tubes were bulky and broke all the time. Imagine what the work of an IT support specialist was like in those early days of computing. The job description might have included crawling around inside huge machines filled with dust and creepy crawly things, or replacing vacuum tubes and swapping out those punch cards. In those days, doing some debugging might've taken on a more literal meaning.

🐛 Well-known computer scientist Admiral Grace Hopper had a favorite story involving some engineers working on the Harvard Mark II computer. They were trying to figure out the source of the problems in a relay. After doing some investigating, they discovered the source of their trouble was a moth, a literal bug in the computer.

## The Age of Transistors and Microprocessors 🏭

🔌 The ENIAC was one of the earliest forms of general-purpose computers. It was a wall-to-wall convolution of massive electronic components and wires. 17,000 vacuum tubes and took up about 1,800 square feet of floor space. Imagine if you had to work with that scale of equipment today, I wouldn't want to share an office with 1,800 square feet of machinery.

🔧 Eventually, the industry started using transistors to control electricity voltages. This is now a fundamental component of all electronic devices. Transistors perform almost the same functions as vacuum tubes, but they are more compact and more efficient. You can easily have billions of transistors in a small computer chip today.

📅 Throughout the decades, more and more advancements were made. The very first compiler was invented by Admiral Grace Hopper. Compilers made it possible to translate human language via a programming language into machine code. The big takeaway is that this advancement was a huge milestone in computing that led to where we are today. Now, learning programming languages is accessible for almost anyone anywhere. We no longer have to learn how to write machine code in ones and zeros.

🖥️ Eventually, the industry gave way to the first hard disk drives and microprocessors. Then programming language started becoming the predominant way for engineers who develop computer software. Computers were getting smaller and smaller, thanks to advancements in electronic components. Instead of filling up entire rooms like ENIAC, they were getting small enough to fit on tabletops.

📚  The Xerox Alto was the first computer that resembled the computers we're familiarwith today. It had a graphical user interface (GUI) and a mouse, which made it easier for users to interact with the computer. This was a significant development as it paved the way for the personal computer revolution.

💻 Over time, computers became more affordable and accessible to the average person. Companies like Apple and Microsoft played a pivotal role in popularizing personal computers and making them user-friendly. The introduction of iconic products like the Apple Macintosh and the Microsoft Windows operating system brought computing to the masses.

📱 Then came the era of mobile computing with the invention of smartphones and tablets. Devices like the iPhone and the iPad revolutionized the way we communicate, work, and access information. These pocket-sized devices with powerful computing capabilities have become an integral part of our daily lives.

🌐 The internet has also played a crucial role in shaping the modern computing landscape. With the advent of the World Wide Web, information became easily accessible to anyone with an internet connection. The internet has connected people from all corners of the globe, enabling communication, collaboration, and the sharing of knowledge on an unprecedented scale.

🚀 Looking ahead, the field of computing continues to evolve rapidly. Emerging technologies like artificial intelligence, virtual reality, and quantum computing hold the promise of transforming our lives in ways we can only imagine.

🔮 In conclusion, from the massive machines and punch cards of the past to the sleek and powerful devices we use today, computing has come a long way. It has become an essential tool in various aspects of our lives, driving innovation, connecting people, and opening up new possibilities. The future of computing looks incredibly exciting, and we can't wait to see what lies ahead.